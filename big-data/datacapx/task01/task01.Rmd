---
title: "Project Task 1"
output: _html_notebook
---

# Web References

- https://laptrinhx.com/text-mining-in-r-a-tutorial-889565373/
- https://statisticsglobe.com/r-error-cannot-allocate-vector-of-size-n-gb
- https://www.kaggle.com/aquaregis32/text-mining-rmarkdown

# Import Libraries

```{r}
library(tidyverse)
library(lubridate)
library(tidytext)
library(tm)
library(SnowballC)
```

# Data Load

1.	Read in the reddit dataset correctly and provide evidence of your code.

```{r}
reddit <- read.csv("RS_2017-09_filtered70.csv", stringsAsFactors=TRUE)
reddit <- as_tibble(reddit)
reddit
```

# Data Clean

2.	Remove the first column of the data frame and provide evidence.

```{r}
reddit <- select(reddit, -X)
reddit
```

3.	Correctly identify and remove the columns that contain no data. Provide a list of the variables removed and evidence of your code. 

## Identify Columns with no data

```{r}
reddit.is_empty <- reddit %>%
  apply(
    MARGIN=2,
    FUN=function(x) all(is.na(x))) %>%
  data.frame

colnames(reddit.is_empty)[1] <- "empty"

reddit.is_empty %>%
  filter(empty == TRUE)
```

## Remove Empty Columns

```{r}
reddit <- reddit[, !sapply(reddit, function(x) all(is.na(x))), drop=FALSE]
reddit
```

4.	Your code correctly identified and removed the columns that contained only one value. Provide a list of the variables removed and evidence of your code. 

## Identify Columns with only one value

```{r}
reddit %>% 
  select_if(~n_distinct(.) == 1) %>%
  head(1)
```
## Remove columns with a single value

```{r}
reddit <- reddit %>% 
  select_if(~n_distinct(.) > 1)
  
reddit
```



```{r}
is.na(reddit) %>%
  as_tibble() %>%
  gather(key="column", value="empty") %>%
  filter(empty == FALSE) %>%
  group_by(column) %>%
  summarise(count=n())
```

5.	Convert the variables created_utc and retrieved_on to unordered factors containing the weekdays. Provide tables of the frequencies for the two new variables and evidence of your code. 

## Convert the columns to abbreviated week day names

```{r}
reddit <- reddit %>%
  mutate(created_utc = as.POSIXct(created_utc, origin = "1970-01-01"))  %>%
  mutate(retrieved_on = as.POSIXct(retrieved_on, origin = "1970-01-01")) %>%
  mutate(created_utc = wday(created_utc, label=TRUE, abbr=TRUE))  %>%
  mutate(retrieved_on = wday(retrieved_on, label=TRUE, abbr=TRUE))
```

## Convert the columns to factors

```{r}
reddit$created_utc <- factor(reddit$created_utc)
reddit$retrieved_on <- factor(reddit$retrieved_on)
```

## summarize created_utc

```{r}
reddit %>%
  select(created_utc) %>%
  group_by(created_utc) %>%
  summarise(freq=n())
```

## summarize retrieved_on

```{r}
reddit %>%
  select(retrieved_on) %>%
  group_by(retrieved_on) %>%
  summarise(freq=n())
```

## Another test

```{r}
#reddit %>%
#  select(created_utc, retrieved_on) %>%
#  mutate(created_utc = as.POSIXct(created_utc, origin = "1970-01-01"))  %>%
#  mutate(retrieved_on = as.POSIXct(retrieved_on, origin = "1970-01-01")) %>%
#  mutate(created_utc = wday(created_utc, label=TRUE, abbr=TRUE))  %>%
#  mutate(retrieved_on = wday(retrieved_on, label=TRUE, abbr=TRUE)) %>%
#  gather(key="column", value="weekday") %>%
#  group_by(column, weekday) %>%
#  summarise(count=n())
```

6.	Your code successfully converts the titles of the reddit posts to an incidence matrix of the words that appear in at least 500 posts. Provide a list of the words that appear in at least 500 posts and incorporate the incidence matrix into the reddit data frame. Provided evidence of your code.

## Step 6.1: Convert all letters to lowercase. 
Step 6.2: Remove numbers.
Step 6.4: Remove punctuation.
Step 6.5: Eliminate extra white space.


```{r}
#reddit.titles <- reddit %>%
#  select(title) %>%
#  mutate(title=tolower(title)) %>%
#  mutate(title=gsub("\\d+", "", title)) %>%
#  mutate(title=gsub("[[:punct:]]", "", title)) %>%
#  mutate(title=gsub("\\s+", " ", str_trim(title)))
#  
#reddit.titles
```



Step 6.3: Remove stop words such as "the".

```{r}
#data(stop_words)
#
#reddit.titles %>%
#  unnest_tokens(word, title) %>%
#  anti_join(stop_words, by = "word")
```



6.	Your code successfully converts the titles of the reddit posts to an incidence matrix of the words that appear in at least 500 posts. Provide a list of the words that appear in at least 500 posts and incorporate the incidence matrix into the reddit data frame. Provided evidence of your code.

## Step 6.1: Convert all letters to lowercase. 


```{r}
docs <- Corpus(VectorSource(pull(reddit, title)))
docs <- tm_map(docs, content_transformer(tolower))
```

## Step 6.3: Remove stop words such as "the".

```{r}
docs <- tm_map(docs, removeWords, stopwords("english"))
```

## Step 6.2: Remove numbers.

```{r}
docs <- tm_map(docs, removeNumbers)
```

## Step 6.4: Remove punctuation.

```{r}
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, content_transformer(function(x) gsub(x, pattern = "–", replacement = " ")))
```

## Step 6.5: Eliminate extra white space.

```{r}
docs <- tm_map(docs, stripWhitespace)
```

## Step 6.6: Reduce all words to their stem words. For example, "car", "car's" and "cars" are all reduced to the stem "car". In some cases, the stem may not be a complete word. For example, “removed” and “removal” might be transformed to a common stem, “remov”.

```{r}
docs <- tm_map(docs, stemDocument)
```


Step 6.7: Represent the data as an incidence matrix with one column for each stem word and one row for each post. The body of the matrix should contain "1" when the title contains the word and "0" otherwise.


```{r}
# get the document term matrix and convert it to an incidence matrix
dtm <- TermDocumentMatrix(docs)
dtm$v <-rep(1, length(dtm$v))
```

Step 6.8: For the purpose of this project, attention will be restricted to words that appear in at least 500 titles. You will need to remove all words that appear less frequently.

```{r}
# find terms that appears at least in 500 titles
ft <- findFreqTerms(dtm, lowfreq=500)

# filter the document term matrix to only frequent terms
dtm<-dtm[Terms(dtm)%in%ft,]
```

Step 6.9: The final result should be a matrix with 139,823 rows and one column for each word that appears at least 500 times. When this is obtained, the titles column should be removed from and the incidence matrix added to the reddit data frame.

```{r}
# convert the dtm to a matrix
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)

pull(d, word)
```

```{r}
# transpose the matrix and add a column prefix
m <- t(m)
colnames(m) <- paste("title", colnames(m), sep = "_")

#add the incident matrix data to the reddit data frame
reddit <- bind_cols(reddit, as_tibble(m), .name_repair="unique")

# remove the title column
reddit <- select(reddit, -title)

colnames(reddit)
```

# --- Recoding of factors

Step 7: You will need to identify all factors where at least one level occurs less than 30 times and recode them accordingly.

```{r}
# identify factors to recode
factor_cols <- reddit %>%
  select_if(is.factor) %>%  
  gather(name, value) %>% 
  count(name, value) %>%
  filter(n < 30) %>%
  group_by(name) %>%
  summarise() %>%
  pull(name)

factor_cols
```

```{r}
# recode the factors found with at least one factor occurring less than 30 times
reddit[factor_cols] <- sapply(reddit[factor_cols], function(x) fct_lump_min(f=x, min=30, other_level="other"))
reddit <- reddit %>% mutate_at(all_of(factor_cols), as.factor)
```


Step 8: To make this analysis manageable, it is suggested for factors with more than 100 levels, that the 100 most commonly occurring levels are retained, and that you amalgamate the remainder to the “other” category.

```{r}
# identify factors with more than 100 levels
many_factor <- reddit %>%
  select_if(is.factor) %>% 
  sapply(nlevels) %>%
  data.frame() %>%
  rownames_to_column() %>%
  setNames(c("name", "n")) %>%
  filter(n > 100) %>%
  pull(name)

many_factor
```
```{r}
# retain only the 100 most commonly occuring levels
reddit[many_factor] <- sapply(reddit[many_factor], function(x) fct_lump_n(f=x, n=100, other_level="other"))
reddit <- reddit %>% mutate_at(all_of(many_factor), as.factor)
```


```{r}
# show factors with more than 100 levels
reddit %>%
  select_if(is.factor) %>% 
  sapply(nlevels) %>%
  data.frame() %>%
  rownames_to_column() %>%
  setNames(c("name", "n")) %>%
  filter(n > 100)
```

```{r}
reddit %>% 
  select(link_flair_css_class) %>%
  group_by(link_flair_css_class) %>%
  summarise(n())
```


Step 9: Some factors cannot be suitably recoded. For example, a binary factor where one level has fewer than 30 occurrences.

```{r}
# find factors that can't be successfully recoded
removal_factors <- reddit %>%
  select_if(is.factor) %>%  
  gather(name, value) %>% 
  count(name, value) %>%
  filter(n < 30) %>%
  group_by(name) %>%
  summarise() %>%
  pull(name)

removal_factors
```
```{r}
# remove factors that could not be recoded
reddit <- reddit %>%
  select(-all_of(removal_factors))
```


```{r}
# find factors with only one level
uniform_factors <- reddit %>%
  select_if(is.factor) %>% 
  sapply(nlevels) %>%
  data.frame() %>%
  rownames_to_column() %>%
  setNames(c("name", "n")) %>%
  filter(n == 1) %>%
  pull(name)

uniform_factors
```

```{r}
# remove uniform factors
reddit <- reddit %>%
  select(-all_of(uniform_factors))
```

# --- Additional considerations and important notes

```{r}
# identify columns with an excessive  number of missing values
na_factors <- reddit %>%
  sapply(function(x) sum(length(which(is.na(x))))) %>%
  data.frame() %>%
  rownames_to_column() %>%
  setNames(c("name", "n")) %>%
  filter(n > 20) %>%
  pull(name)

na_factors
```
```{r}
# remove factors with excessive NA values
reddit <- reddit %>%
  select(-all_of(na_factors))
```

11.	Provide a list of the remaining factors and the number of levels in the reddit data frame at the end of the data cleaning steps.

```{r}
# list remaining factors and number of levels
reddit %>%
  select_if(is.factor) %>% 
  sapply(nlevels) %>%
  data.frame() %>%
  rownames_to_column() %>%
  setNames(c("factor", "levels"))
```

# --- Data transformation

```{r}
# identify numeric predictor variables
numeric_columns <- reddit %>% 
  select(where(is.numeric)) %>%
  select(-starts_with("title_")) %>%
  names()

numeric_columns
```


```{r}
# get the numeric column values to plot
numeric_data <- reddit %>% 
  select(all_of(numeric_columns)) %>%
  gather(key="column", value="value", -score)

head(numeric_data)
```

```{r}
# temportatu test
#numeric_data <- reddit.clean %>% 
#  select(score, gilded, num_comments, num_crossposts) %>%
#  gather(key="column", value="value", -score)

#head(numeric_data)
```


```{r}
# plot the unchanged numeric values against score
ggplot(numeric_data, aes(x=value, y=score)) + 
  stat_bin_hex(bins=10) + 
  scale_fill_continuous(low="#B6D191", high="darkgreen") +
  geom_smooth(method='lm', color="darkgray") +
  facet_wrap(~column, scales="free")
```

```{r}
# transform the numeric values
scale <- function(x) log(1 + x)

scaled_data <- numeric_data %>%
  mutate_if(is.numeric, scale)

scaled_data
```

```{r}
# plot the scaled numeric values against score
ggplot(scaled_data, aes(x=value, y=score)) + 
  stat_bin_hex(bins=10) +
  scale_fill_continuous(low="#B6D191", high="darkgreen") +
  geom_smooth(method='lm', color="darkgray") +
  facet_wrap(~column, scales="free")
```

```{r}
# transform the numeric values in the data set
reddit <- reddit %>% mutate_at(all_of(numeric_columns), scale)
```

